# Ollama Chat Interface

This directory contains a Python script (`chat.py`) and a Flask web application (`app.py` inside the `flask` folder) that facilitate interaction with different AI models provided by Ollama. These tools create a user-friendly way to chat with AI models directly from the command line or through a browser.

## Standalone Python App (chat.py)

### Features:

- **Multiple AI Models**: Chat with Llama2, Gemma 2B, Gemma 7B, or Mistral models by choosing them interactively.
- **Error Handling**: Handles the 'model not found' error by advising to pull the necessary AI model. 
- **User Interaction**: Provides an interactive prompt in the terminal to send messages and receive responses from the chosen AI model.

### Setup:

Before running `chat.py` for the first time, ensure all models are available locally by executing `ollama pull` commands for each model listed in the script.

### Usage:

Execute `python chat.py` in the terminal, then interact with the prompt to chat with AI models. Type 'quit' to exit the chat or 'switch' to change the AI model during the conversation.

## Flask Web App (ollama/flask/app.py)

### Features:

- **Web Interface**: Interact with AI models through a minimalistic web interface accessible from local servers.
- **Asynchronous Chat**: Utilizes XMLHttpRequests in JavaScript for seamless message sending and receiving without reloading the page.
- **User Input Handling**: Provides straightforward options for model selection and text input for user messages.

### Setup:

No extra setup is needed except running `ollama pull` commands if they haven't been run before, similar to the standalone app.

### Usage:

Navigate to the `ollama/flask` directory and run `app.py` with `python app.py`. Visit `http://localhost:5000` in your web browser and access the chat interface to start interacting with the AI models.

## Directory Structure

The directory layout for the Ollama chat interface is organized as follows:
kumori_ai/ └── ollama/ ├── chat.py # Standalone Python chat app ├── flask/ │ ├── app.py # Flask web application │ └── templates/ │ └── ollama.html # HTML template for the Flask app └── pycache/ # Python cache directory (autogenerated)


## Important Information

The standalone `chat.py` should be run in a terminal. Meanwhile, the Flask application `app.py` serves a web interface that can be opened in a browser for a more interactive experience. The backend functionality handling the conversation with AI models is shared between the two, ensuring a consistent chat experience regardless of the interface used.

Please note that if the models need an update or haven't been pulled yet, the scripts might not function as intended. Make sure to keep the Ollama client and models up to date for the best user experience.

With these tools, interacting with advanced AI chat models has never been easier, whether you're a developer looking to integrate AI into your applications or just someone curious about conversing with AI.